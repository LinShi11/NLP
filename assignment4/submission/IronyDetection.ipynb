{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignment (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will solve an irony detection task: given a tweet, your job is to classify whether it is ironic or not.\n",
    "\n",
    "You will implement a new classifier that does not rely on feature engineering as in previous homeworks. Instead, you will use pretrained word embeddings downloaded from using the `irony.py` script as your input feature vectors. Then, you will encode your sequence of word embeddings with an (already implemented) LSTM and classify based on its final hidden state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use the dataset from SemEval-2018: https://github.com/Cyvhee/SemEval2018-Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from irony import load_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, train_labels, test_sentences, test_labels, label2i = load_datasets()\n",
    "\n",
    "text_train, text_dev, label_train, label_dev = train_test_split(train_sentences, train_labels, test_size = 0.2, stratify = train_labels) \n",
    "\n",
    "# TODO: Split train into train/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Naive Bayes\n",
    "\n",
    "We have provided the solution for the Naive Bayes part from HW2 in [bayes.py](bayes.py)\n",
    "\n",
    "There are two implementations: NaiveBayesHW2 is what was expected from HW2. However, we will use a more effecient implementation of it that uses vector operations to calculate the probabilities. Please go through it if you would like to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing Text: 100%|██████████████████████████████████████████████████████████| 3834/3834 [00:00<00:00, 9574.52it/s]\n",
      "Vectorizing Text: 100%|█████████████████████████████████████████████████████████| 3834/3834 [00:00<00:00, 11618.50it/s]\n",
      "Vectorizing Text: 100%|███████████████████████████████████████████████████████████| 784/784 [00:00<00:00, 15521.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: Naive Bayes Classifier\n",
      "F1-score Ironic: 0.6402966625463535\n",
      "Avg F1-score: 0.6284487265300938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from irony import run_nb_baseline\n",
    "\n",
    "run_nb_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement avg_f1_score() in [util.py](util.py). Then re-run the above cell  (2 Points)\n",
    "\n",
    "So the micro F1-score for the test set of the Ironic Class using a Naive Bayes Classifier is **0.64**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Word2Vec  (Total: 18 Points)\n",
    "\n",
    "Unlike sentiment, Irony is very subjective, and there is no word list for ironic and non-ironic tweets. This makes hand-engineering features tedious, therefore, we will use word embeddings as input to the classifier, and make the model automatically extract features aka learn weights for the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer for Tweets\n",
    "\n",
    "\n",
    "Tweets are very different from normal document text. They have emojis, hashtags and bunch of other special character. Therefore, we need to create a suitable tokenizer for this kind of text.\n",
    "\n",
    "Additionally, as described in class, we also need to have a consistent input length of the text document in order for the neural networks built over it to work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create a Tokenizer with Padding (5 Points)\n",
    "\n",
    "Our Tokenizer class is meant for tokenizing and padding batches of inputs. This is done\n",
    "before we encode text sequences as torch Tensors.\n",
    "\n",
    "Update the following class by completing the todo statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self, pad_symbol: Optional[str] = \"<PAD>\"):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<PAD>\".\n",
    "        \"\"\"\n",
    "        self.pad_symbol = pad_symbol\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def __call__(self, batch: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes each sentence in the batch, and pads them if necessary so\n",
    "        that we have equal length sentences in the batch.\n",
    "\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A List of equal-length token Lists.\n",
    "        \"\"\"\n",
    "        batch = self.tokenize(batch)\n",
    "        batch = self.pad(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def tokenize(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes the List of string sentences into a Lists of tokens using spacy tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (List[str]): The input sentence.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The tokenized version of the sentence.\n",
    "        \"\"\"\n",
    "        # TODO: Tokenize the input with spacy.\n",
    "        # TODO: Make sure the start token is the special <SOS> token and the end token\n",
    "        #       is the special <EOS> token\n",
    "        # raise NotImplementedError\n",
    "        tokenized_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = [\"<SOS>\"] + [token.text for token in self.nlp(sentence)] + [\"EOS\"]\n",
    "            tokenized_sentences.append(tokens)\n",
    "\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def pad(self, batch: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"Appends pad symbols to each tokenized sentence in the batch such that\n",
    "        every List of tokens is the same length. This means that the max length sentence\n",
    "        will not be padded.\n",
    "\n",
    "        Args:\n",
    "            batch (List[List[str]]): Batch of tokenized sentences.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: Batch of padded tokenized sentences. \n",
    "        \"\"\"\n",
    "        # TODO: For each sentence in the batch, append the special <P>\n",
    "        #       symbol to it n times to make all sentences equal length\n",
    "        # raise NotImplementedError\n",
    "        max_len = max(len(sentence) for sentence in batch)\n",
    "        padded_batch = [sentence + [self.pad_symbol] * (max_len - len(sentence)) for sentence in batch]\n",
    "\n",
    "        return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vocabulary of the dataset: use both training and test sets here\n",
    "\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>']\n",
    "\n",
    "all_data = train_sentences + test_sentences\n",
    "my_tokenizer = Tokenizer()\n",
    "\n",
    "tokenized_data = my_tokenizer.tokenize(all_data)\n",
    "vocab = sorted(set([w for ws in tokenized_data + [SPECIAL_TOKENS] for w in ws]))\n",
    "\n",
    "with open('vocab.txt', 'w', encoding = 'utf-8') as vf:\n",
    "    vf.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We use GloVe embeddings https://nlp.stanford.edu/projects/glove/. But these do not necessarily have all of the tokens that will occur in tweets! Hoad the GloVe embeddings, pruning them to only those words in vocab.txt. This is to reduce the memory and runtime of your model.\n",
    "\n",
    "Then, find the out-of-vocabulary words (oov) and add them to the encoding dictionary and the embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload the gloVe vectors for Twitter tweets. This will download a file called glove.twitter.27B.zip\n",
    "\n",
    "# ! python -m wget https://nlp.stanford.edu/data/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip glove.twitter.27B.zip\n",
    "# if there is an error, please download the zip file again\n",
    "# Manually unzipped based on piazza post\n",
    "# ! unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what files are there:\n",
    "# 4 different files\n",
    "# ! ls . | grep \"glove.*.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this assignment, we will use glove.twitter.27B.50d.txt which has 50 dimensional word vectors\n",
    "# Feel free to experiment with vectors of other sizes\n",
    "\n",
    "embeddings_path = 'glove.twitter.27B.50d.txt'\n",
    "vocab_path = \"./vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom Embedding Layer\n",
    "\n",
    "Now the GloVe file has vectors for about 1.2 million words. However, we only need the vectors for a very tiny fraction of words -> the unique words that are there in the classification corpus. Some of the next tasks will be to create a custom embedding layer that has the vectors for this small set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Extracting word vectors from GloVe (3 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab_path: str\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Note that we have provided the entire vocab for train and test, so that for practical purposes\n",
    "    we can simply load those words in the vocab, rather than all 27B embeddings\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    with open(vocab_path, encoding='utf8') as vf:\n",
    "        vocab = set([w.strip() for w in vf.readlines()]) \n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\", encoding='utf8') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            # TODO: Build word2i and vectors such that\n",
    "            #       each word points to the index of its vector,\n",
    "            #       and only words that exist in `vocab` are in our embeddings\n",
    "            # raise NotImplementedError\n",
    "            if word in vocab:\n",
    "                word2i[word] = len(word2i)\n",
    "                vector = torch.FloatTensor([float(weight) for weight in weights])\n",
    "                vectors.append(vector)\n",
    "\n",
    "    return word2i, torch.stack(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Get GloVe Out of Vocabulary (oov) words (0 Points)\n",
    "\n",
    "The task is to find the words in the Irony corpus that are not in the GloVe Word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oovs(vocab_path: str, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    with open(vocab_path, encoding='utf8') as vf:\n",
    "        vocab = set([w.strip() for w in vf.readlines()])\n",
    "    \n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = vocab - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Update the embeddings with oov words (3 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_new_embedding_weights(num_embeddings: int, dim: int) -> torch.FloatTensor:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    # TODO: Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    #      That is, a normal distribution with mean 0 and standard deviation of dim^-0.5\n",
    "    # raise NotImplementedError\n",
    "    std_dev = dim ** -0.5\n",
    "    return torch.randn(num_embeddings, dim) / std_dev\n",
    "\n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: torch.FloatTensor,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    # TODO: Add the oov words to the dict, assigning a new index to each\n",
    "\n",
    "    # TODO: Concatenate a new row to embeddings for each oov\n",
    "    #       initialize those new rows with `intialize_new_embedding_weights`\n",
    "\n",
    "    # TODO: Return the tuple of the dictionary and the new embeddings matrix\n",
    "    # raise NotImplementedError\n",
    "    word2i = glove_word2i.copy()\n",
    "    embeddings = glove_embeddings.clone()\n",
    "    for oov in oovs:\n",
    "        if oov not in word2i:\n",
    "            word2i[oov] = len(word2i)\n",
    "\n",
    "    new_rows = intialize_new_embedding_weights(len(oovs), embeddings.size(1))\n",
    "    embeddings = torch.cat([embeddings, new_rows], dim=0)\n",
    "    return word2i, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from glove.twitter.27B.50d.txt...\n"
     ]
    }
   ],
   "source": [
    "def make_batches(sequences: List[str], batch_size: int) -> List[List[str]]:\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "    # TODO\n",
    "    # raise NotImplementedError\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        yield sequences[i:i+ batch_size]\n",
    "\n",
    "\n",
    "# TODO: Set your preferred batch size\n",
    "batch_size = 8\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# We make batches now and use those.\n",
    "batch_tokenized = []\n",
    "# Note: Labels need to be batched in the same way to ensure\n",
    "# We have train sentence and label batches lining up.\n",
    "for batch in make_batches(train_sentences, batch_size):\n",
    "    batch_tokenized.append(tokenizer(batch))\n",
    "\n",
    "\n",
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(\n",
    "    embeddings_path,\n",
    "    vocab_path\n",
    ")\n",
    "\n",
    "# Find the out-of-vocabularies\n",
    "oovs = get_oovs(vocab_path, glove_word2i)\n",
    "\n",
    "# Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# to the embeddings matrix\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding words to integers: DO NOT EDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these functions to encode your batches before you call the train loop.\n",
    "\n",
    "def encode_sentences(batch: List[List[str]], word2i: Dict[str, int]) -> torch.LongTensor:\n",
    "    \"\"\"Encode the tokens in each sentence in the batch with a dictionary\n",
    "\n",
    "    Args:\n",
    "        batch (List[List[str]]): The padded and tokenized batch of sentences.\n",
    "        word2i (Dict[str, int]): The encoding dictionary.\n",
    "\n",
    "    Returns:\n",
    "        torch.LongTensor: The tensor of encoded sentences.\n",
    "    \"\"\"\n",
    "    UNK_IDX = word2i[\"<UNK>\"]\n",
    "    tensors = []\n",
    "    for sent in batch:\n",
    "        tensors.append(torch.LongTensor([word2i.get(w, UNK_IDX) for w in sent]))\n",
    "        \n",
    "    return torch.stack(tensors)\n",
    "\n",
    "\n",
    "def encode_labels(labels: List[int]) -> torch.FloatTensor:\n",
    "    \"\"\"Turns the batch of labels into a tensor\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): List of all labels in the batch\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Tensor of all labels in the batch\n",
    "    \"\"\"\n",
    "    return torch.LongTensor([int(l) for l in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling   ( 7 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Notice there is a single TODO in the model\n",
    "class IronyDetector(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        embeddings_tensor: torch.FloatTensor,\n",
    "        pad_idx: int,\n",
    "        output_size: int,\n",
    "        dropout_val: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        self.dropout_val = dropout_val\n",
    "        self.output_size = output_size\n",
    "        # TODO: Initialize the embeddings from the weights matrix.\n",
    "        #       Check the documentation for how to initialize an embedding layer\n",
    "        #       from a pretrained embedding matrix. \n",
    "        #       Be careful to set the `freeze` parameter!\n",
    "        #       Docs are here: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(embeddings_tensor)#, freeze=False, padding_idx=self.pad_idx)\n",
    "        # Dropout regularization\n",
    "        # https://jmlr.org/papers/v15/srivastava14a.html\n",
    "        self.dropout_layer = torch.nn.Dropout(p=self.dropout_val, inplace=False)\n",
    "        # Bidirectional 2-layer LSTM. Feel free to try different parameters.\n",
    "        # https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            self.input_dim,\n",
    "            self.hidden_dim,\n",
    "            num_layers=2,\n",
    "            dropout=dropout_val,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        # For classification over the final LSTM state.\n",
    "        self.classifier = torch.nn.Linear(hidden_dim*2, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "        Args:\n",
    "            symbols (torch.Tensor): The batch size x sequence length tensor of input tokens\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "                the entire sentence\n",
    "        \"\"\"\n",
    "\n",
    "        if(symbols.dim() == 1):\n",
    "            symbols = symbols.unsqueeze(0)\n",
    "        \n",
    "        # First we get the embedding for each input symbol\n",
    "        embedded = self.embeddings(symbols)\n",
    "        embedded = self.dropout_layer(embedded)\n",
    "        \n",
    "        # Packs embedded source symbols into a PackedSequence.\n",
    "        # This is an optimization when using padded sequences with an LSTM\n",
    "        \n",
    "        lens = (symbols != self.pad_idx).sum(dim=1).to(\"cpu\")\n",
    "        lens = torch.maximum(lens, torch.tensor(1, dtype=lens.dtype))\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lens, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # -> batch_size x seq_len x encoder_dim, (h0, c0).\n",
    "        packed_outs, (H, C) = self.lstm(packed)\n",
    "        encoded, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outs,\n",
    "            batch_first=True,\n",
    "            padding_value=self.pad_idx,\n",
    "            total_length=None,\n",
    "        )\n",
    "        # Now we have the representation of each token encoded by the LSTM.\n",
    "        encoded, (H, C) = self.lstm(embedded)\n",
    "        \n",
    "        # This part looks tricky. All we are doing is getting a tensor\n",
    "        # That indexes the last non-PAD position in each tensor in the batch.\n",
    "        last_enc_out_idxs = lens - 1\n",
    "        # -> B x 1 x 1.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.view([encoded.size(0)] + [1, 1])\n",
    "        # -> 1 x 1 x encoder_dim. This indexes the last non-padded dimension.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.expand(\n",
    "            [-1, -1, encoded.size(-1)]\n",
    "        )\n",
    "        # Get the final hidden state in the LSTM\n",
    "        last_hidden = torch.gather(encoded, 1, last_enc_out_idxs)\n",
    "        return last_hidden\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.classifier(encoded_sents)\n",
    "        return self.log_softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: torch.nn.Module, dev_sequences: List[torch.Tensor]):\n",
    "    preds = []\n",
    "    # TODO: Get the predictions for the dev_sequences using the model\n",
    "    # raise NotImplementedError\n",
    "    preds = []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for symbols in dev_sequences:\n",
    "            output = model(symbols)\n",
    "            \n",
    "            _, predicted_labels = torch.max(output, 2)\n",
    "            preds.append(predicted_labels.tolist())\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "from util import avg_f1_score, f1_score\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_features,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for features, labels in tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features).squeeze(1)\n",
    "            labels = labels.unsqueeze(0)\n",
    "            loss = loss_func(preds, labels)\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        preds = predict(model, dev_features)\n",
    "\n",
    "        # due to my previous code, the dimension is not completely correct, flat it out\n",
    "        flat_preds = []\n",
    "        for element in preds:\n",
    "            flat_preds.append(element[0][0])\n",
    "        dev_f1 = f1_score(flat_preds, dev_labels, label2i['1'])\n",
    "        dev_avg_f1 = avg_f1_score(flat_preds, dev_labels, list(label2i.keys()))\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        print(f\"Avf Dev F1 {dev_avg_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linsh\\AppData\\Local\\Temp\\ipykernel_130784\\587731332.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for features, labels in tqdm(batches):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f28bc304f44e06bdbf5b4bbece8dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.7454035960436645\n",
      "Evaluating dev...\n",
      "Dev F1 0.5680365296803653\n",
      "Avf Dev F1 0.28401826484018267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb571e1f6064eb8b6f304e14d8ccc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.7520697910786841\n",
      "Evaluating dev...\n",
      "Dev F1 0.5685557586837293\n",
      "Avf Dev F1 0.28638758398321484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d1c99ef8114146ae7015495ab3837c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 0.7851492331642931\n",
      "Evaluating dev...\n",
      "Dev F1 0.5680365296803653\n",
      "Avf Dev F1 0.28401826484018267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502f12b6585144b5812784d363688515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 0.7776615808365358\n",
      "Evaluating dev...\n",
      "Dev F1 0.5680365296803653\n",
      "Avf Dev F1 0.28401826484018267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9b202f7dbb4bf89c38e010bd695682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 0.7767891655534865\n",
      "Evaluating dev...\n",
      "Dev F1 0.5680365296803653\n",
      "Avf Dev F1 0.28401826484018267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee8582007e240bb867449c52221f2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss: 0.7769248262667232\n",
      "Evaluating dev...\n",
      "Dev F1 0.5722171113155474\n",
      "Avf Dev F1 0.30274057228979034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72cbd22a20f4e2992b64f4c913c2704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss: 0.7573146273825047\n",
      "Evaluating dev...\n",
      "Dev F1 0.601010101010101\n",
      "Avf Dev F1 0.59689680308237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b29e85dbc154e09ba8c12f585dc8391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss: 0.7408549033542187\n",
      "Evaluating dev...\n",
      "Dev F1 0.5889101338432123\n",
      "Avf Dev F1 0.3825776722855908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d8196d1d734b1aafd5b21de9a32aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss: 0.7272581803157834\n",
      "Evaluating dev...\n",
      "Dev F1 0.40704500978473585\n",
      "Avf Dev F1 0.560192325138347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aba7b02cd334972b30db9410aa44ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss: 0.746205808693512\n",
      "Evaluating dev...\n",
      "Dev F1 0.5473411154345006\n",
      "Avf Dev F1 0.5547245100384548\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9f585e065f4f91b989c2eabdb88ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 0.722943169026331\n",
      "Evaluating dev...\n",
      "Dev F1 0.560398505603985\n",
      "Avf Dev F1 0.5494802985536265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f424375627234107829f157a512fcde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 0.7387737606779694\n",
      "Evaluating dev...\n",
      "Dev F1 0.4258289703315881\n",
      "Avf Dev F1 0.547587851999965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d184d799eb473883041ab90ec00aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss: 0.7390346753638056\n",
      "Evaluating dev...\n",
      "Dev F1 0.5495118549511855\n",
      "Avf Dev F1 0.5849791942205986\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6946c905ad4cc8a55f6856367e1798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss: 0.7287901155534466\n",
      "Evaluating dev...\n",
      "Dev F1 0.40661157024793393\n",
      "Avf Dev F1 0.5169091080730843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f76fda15ff946e7b2d318b22652f570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss: 0.7305173016754369\n",
      "Evaluating dev...\n",
      "Dev F1 0.38966202783300197\n",
      "Avf Dev F1 0.55069955851744\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6b66e534004b9e8362a64874526a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, loss: 0.7227549278122662\n",
      "Evaluating dev...\n",
      "Dev F1 0.10899182561307902\n",
      "Avf Dev F1 0.41835935993393325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c816cf831944064b6ec26636e3f4b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, loss: 0.7385619107997006\n",
      "Evaluating dev...\n",
      "Dev F1 0.45407279029462744\n",
      "Avf Dev F1 0.5681060217870716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfcec410f614f389cc826c332c8fcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, loss: 0.7377002371262084\n",
      "Evaluating dev...\n",
      "Dev F1 0.49087893864013266\n",
      "Avf Dev F1 0.586372111807113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c12d784d8e4ae587daab2b8edeb17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, loss: 0.7097178135528179\n",
      "Evaluating dev...\n",
      "Dev F1 0.3230088495575221\n",
      "Avf Dev F1 0.5244076505852127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709971522b604878be04c027c79e4aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, loss: 0.7486619811868256\n",
      "Evaluating dev...\n",
      "Dev F1 0.48682170542635655\n",
      "Avf Dev F1 0.5641042438291046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IronyDetector(\n",
       "  (embeddings): Embedding(17302, 50)\n",
       "  (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(50, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the model and run the training loop \n",
    "#       on your train/dev splits. Set and tweak hyperparameters.\n",
    "epochs = 20\n",
    "\n",
    "input_dim = 50\n",
    "hidden_dim = 256 \n",
    "output_size = 2  \n",
    "dropout_val = 0.3  \n",
    "LR = 0.005\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Tokenize and encode training data\n",
    "text_train_tokenized = tokenizer(text_train)\n",
    "text_train_encoded = encode_sentences(text_train_tokenized, word2i)\n",
    "label_train_encoded = encode_labels(label_train)\n",
    "\n",
    "# Tokenize and encode dev data\n",
    "text_dev_tokenized = tokenizer(text_dev)\n",
    "text_dev_encoded = encode_sentences(text_dev_tokenized, word2i)\n",
    "label_dev_encoded = encode_labels(label_dev)\n",
    "\n",
    "model = IronyDetector(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    embeddings_tensor=embeddings, \n",
    "    pad_idx=word2i[\"<PAD>\"], \n",
    "    output_size=output_size,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "\n",
    "# training_loop(\n",
    "#     epochs,\n",
    "#     text_train_encoded, \n",
    "#     label_train_encoded, \n",
    "#     text_dev_encoded, \n",
    "#     label_dev_encoded, \n",
    "#     optimizer,\n",
    "#     model,\n",
    "# )\n",
    "\n",
    "# Tokenize and encode test data\n",
    "text_test_tokenized = tokenizer(test_sentences)\n",
    "text_test_encoded = encode_sentences(text_test_tokenized, word2i)\n",
    "label_test_encoded = encode_labels(test_labels)\n",
    "\n",
    "training_loop(\n",
    "    epochs,\n",
    "    text_train_encoded,  \n",
    "    label_train_encoded, \n",
    "    text_test_encoded,  \n",
    "    label_test_encoded, \n",
    "    optimizer,\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Assignment (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Describe what the task is, and how it could be useful.\n",
    "\n",
    "The task involves irony detection in tweets. Given a tweet, the goal is to classify whether it is ironic or not. This is a binary classification problem where the model needds to distinguish between ironic and non-ironic statements. Irony detection is valuable in various NLP tasks such as sentiment analysis, which could be used to improve user understanding in different settings such as social media analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe, at the high level, that is, without mathematical rigor, how pretrained word embeddings like the ones we relied on here are computed. Your description can discuss the Word2Vec class of algorithms, GloVe, or a similar method.\n",
    "\n",
    "Pretrained word embeddings are computed using algorithms like Word2Vec, GloVe, or similar methods. These embeddings capture the semantic relationships between words based on the different senses. For example, Word2Vec is based on the idea that words appearing in similar contexts have similar meanings. Therefore, using the skip-gram algorithm puts words that appear together as positive examples. Similarly, GloVe aims to capture word relationships based on co-occurrence probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What are some of the benefits of using word embeddings instead of e.g. a bag of words?\n",
    "\n",
    "Word embeddings capture the semantic relationships between words, while bag of words usually lacks the semantic understanding. Additionally, word embedding can represent words in continuous vector space, which may reduce the high dimensionality of one-hot encoded bag-of-words. Lastly, the embeddings will consider the context of words, preserving information about how words are related to each other in the context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the difference between Binary Cross Entropy loss and the negative log likelihood loss we used here (`torch.nn.NLLLoss`)?\n",
    "\n",
    "Binary Cross Entropy Loss, like the name suggest are commonly used for binary classification programs, it measures the difference between predicted probabilities and the actual labels. On the other hand, NNL Loss is used when the model outputs log probabilities. It is more suitable for multiclass classification. In this context, it is applied to sequence to sequecne model where each word is treated as a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Show your experimental results. Indicate any changes to hyperparameters, data splits, or architectural changes you made, and how those effected results.\n",
    "\n",
    "| Epochs | LR     | Hidden Size | Loss         | F1 Score    | Average F1 Score | \n",
    "| ------ | ------ | ----------- | ------------ | ----------- | ---------------- |\n",
    "|   20   | 0.01   | 64          | 0.7041157071 | 0.671065033 | 0.4666107406     |\n",
    "|   20   | 0.005  | 64          | 0.5170861719 | 0.6364846871| 0.6439128416     |\n",
    "|   20   | 0.001  | 64          | 0.1032263312 | 0.6456692913| 0.6479641793     |\n",
    "|   20   | 0.001  | 128         | 0.07466707891| 0.6438896189| 0.6466537357     |\n",
    "|   20   | 0.005  | 128         | 0.5772615606 | 0.6833855799| 0.5791277986     |\n",
    "|   20   | 0.005  | 256         | 0.7922084286 | 0.6749335695| 0.384380365     |\n",
    "|   20   | 0.001  | 256         | 0.07517395022 | 0.6641975309| 0.6442534616     |\n",
    "|   40   | 0.001  | 128         | 0.03310638676 | 0.6837387964| 0.677858774     |\n",
    "|   40   | 0.001  | 256         | 0.05707819213 | 0.6421319797| 0.632057947     |\n",
    "|   20   | 0.001  | 512         | 0.09581331675 | 0.595862068965517| 0.6168432718     |\n",
    "\n",
    "First, I started by randomly assigning values for each of the hyperparameters. Then I started fine-tuning them based on the training and dev data. The above are some of the sample runs during the experiment. \n",
    "\n",
    "#### Learning Rate (LR)\n",
    "I tested various learning rates and their effects on the F1 and average F1 scores. Surprisingly, the F1 score has been consistent, around 0.6 to 0.7; however, there is a variation in the Average F1 score; therefore, I decided that the learning rate will be around 0.001 to 0.005. Given enough time, I would perform a gid search from 0.001 to 0.005 to identify the most optimal learning rate. \n",
    "\n",
    "#### Hidden Size\n",
    "I tested different hidden sizes. Surprisingly, it did not heavily affect the F1 or the average F1 score. One of the test cases for 256 hidden size had the worst average F1 score. However, rerunning the test yields a different result. Therefore, at least in this task, hidden size does not contribute heavily to the F1 scores.  \n",
    "\n",
    "#### Epochs\n",
    "The further examine why the F1 scores are similar, I started increasing the epochs to see how the scores are changing over time. It was clear how the score would suddenly decrease despite the loss still decreasing. Some of the potential explanations could be overfitting or being present in local optimal.\n",
    "\n",
    "#### Other hyperparameters\n",
    "I used the default value in the other hyperparameters as shown in other code portions. For example, the output_size = 2 and dropout_val = 0.3.  \n",
    "\n",
    "#### Data Split\n",
    "Similar to past homework, I used the sklearn built-in, train_test_split(), to split the data into 80-20 splits.\n",
    "\n",
    "#### Other Changes\n",
    "I had to make various code changes to make the program executable and produce results. However, there should be no architectural change to the code itself. \n",
    "\n",
    "#### Final Discussion\n",
    "In the end, without any training on the test data, we obtained 0.48682170542635655 F1 score and 0.5641042438291046 average F1 score. This could indicate that the model is overfitting on the training data. This means we will likely need to do another random start to identify some other starting point for the hyperparameters. However, even though this model trains faster than part A, it still takes considerable time, making additional test work for the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
