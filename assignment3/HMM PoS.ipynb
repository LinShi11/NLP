{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566d8853",
   "metadata": {},
   "source": [
    "# Part A: Parts of Speech Tagging using Hidden Markov Model and Viterbi Algorithm on Hindi Dataset (Total: 40 Points out of 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ae368",
   "metadata": {},
   "source": [
    "For this assignment, we will implement the Viterbi Decoder using the Forward Algorithm of Hidden Markov Model as explained in class.\n",
    "\n",
    "Then, we will create an HMM-based PoS Tagger for Hindi language using the annotated Tagset in nltk.indian\n",
    "\n",
    "You need to first implement the missing code in hmm.py, then run the cells here to get the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71206f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linsh\\AppData\\Local\\Temp\\ipykernel_13136\\987820437.py:1: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850e32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ea92d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\linsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from hmm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f07b6",
   "metadata": {},
   "source": [
    "## 1st-Order Hidden Markov Model Class:\n",
    "\n",
    "The hidden markov model class would have the following attributes: \n",
    "    \n",
    "    1. initial state log-probs vector (pi)\n",
    "    2. state transition log-prob matrix (A)\n",
    "    3. observation log-prob matrix (B)\n",
    "\n",
    "The following methods:\n",
    "    \n",
    "    1. fit method to count the probabilitis of the training set\n",
    "    2. path probability\n",
    "    3. viterbi decoding algorithm\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed8034",
   "metadata": {},
   "source": [
    "## Task 1: Testing the HMM (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9667d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the fit function of the HMM\n",
      "All Test Cases Passed!\n",
      "Testing the decode function of the HMM\n",
      "All Test Cases Passed!\n",
      "Yay! You have a working HMM. Now try creating a pos-tagger using this class.\n"
     ]
    }
   ],
   "source": [
    "### DO NOT EDIT ###\n",
    "\n",
    "# 5 points for the fit test case\n",
    "# 15 points for the decode test case\n",
    "\n",
    "# run the funtion that tests the HMM with synthetic parameters!\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2820a72",
   "metadata": {},
   "source": [
    "## Task 2: PoS Tagging on Hindi Tagset (20 Points)\n",
    "\n",
    "For this assignment, we will use the Hindi Tagged Dataset available with nltk.indian\n",
    "\n",
    "Helper methods to load the dataset is provided in hmm.py\n",
    "\n",
    "Please go through the functions and explore the dataset\n",
    "\n",
    "Report the Accuracy for the Dev and Test Sets. You should get something between 65-85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8564f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id of the <unk> token: 2186\n"
     ]
    }
   ],
   "source": [
    "words, tags, observation_dict, state_dict, all_observation_ids, all_state_ids = get_hindi_dataset()\n",
    "\n",
    "# we need to add the id for unknown word (<unk>) in our observations vocab\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "observation_dict[UNK_TOKEN] = len(observation_dict)\n",
    "print(\"id of the <unk> token:\", observation_dict[UNK_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44817041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique words in the corpus: 2187\n",
      "No. of tags in the corpus 26\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of unique words in the corpus:\", len(observation_dict))\n",
    "print(\"No. of tags in the corpus\", len(state_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1bfe304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432 54 54\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation and development sets\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_indices = list(range(len(all_observation_ids)))\n",
    "\n",
    "train_indices, dev_indices = train_test_split(data_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "dev_indices, test_indices = train_test_split(dev_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "print(len(train_indices), len(dev_indices), len(test_indices))\n",
    "\n",
    "\n",
    "def get_state_obs(state_ids, obs_ids, indices):\n",
    "    return [state_ids[i] for i in indices], [obs_ids[i] for i in indices]\n",
    "\n",
    "\n",
    "train_state_ids, train_observation_ids = get_state_obs(all_state_ids, all_observation_ids, train_indices)\n",
    "dev_state_ids, dev_observation_ids = get_state_obs(all_state_ids, all_observation_ids, dev_indices)\n",
    "test_state_ids, test_observation_ids = get_state_obs(all_state_ids, all_observation_ids, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b197ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unk_id(observation_ids, unk_id, ratio=0.05):\n",
    "    \"\"\"\n",
    "    make 1% of observations unknown\n",
    "    \"\"\"\n",
    "    for obs in observation_ids:\n",
    "        for i in range(len(obs)):\n",
    "            if random.random() < ratio:\n",
    "                obs[i] = unk_id\n",
    "\n",
    "add_unk_id(train_observation_ids, observation_dict[UNK_TOKEN])\n",
    "add_unk_id(dev_observation_ids, observation_dict[UNK_TOKEN])\n",
    "add_unk_id(test_observation_ids, observation_dict[UNK_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "800b6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = HMM(len(state_dict), len(observation_dict))\n",
    "pos_tagger.fit(train_state_ids, train_observation_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb700b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Test Cases Passed!\n"
     ]
    }
   ],
   "source": [
    "assert np.round(np.exp(pos_tagger.pi).sum()) == 1\n",
    "assert np.round(np.exp(pos_tagger.A).sum()) == len(state_dict)\n",
    "assert np.round(np.exp(pos_tagger.B).sum()) == len(state_dict)\n",
    "\n",
    "print('All Test Cases Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d7e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(my_pos_tagger, observation_ids, true_labels):\n",
    "    tag_predictions = my_pos_tagger.decode(observation_ids)\n",
    "    tag_predictions = np.array([t for ts in tag_predictions for t in ts])\n",
    "    true_labels_flat = np.array([t for ts in true_labels for t in ts])\n",
    "    acc = np.sum(tag_predictions == true_labels_flat)/len(tag_predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e418f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev accuracy: 0.8127659574468085\n"
     ]
    }
   ],
   "source": [
    "print('dev accuracy:', accuracy(pos_tagger, dev_observation_ids, dev_state_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a210bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.7987012987012987\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy:', accuracy(pos_tagger, test_observation_ids, test_state_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b83725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a pos_tagger on the entire dataset.\n",
    "import pickle\n",
    "\n",
    "full_state_ids = train_state_ids + dev_state_ids + test_state_ids\n",
    "full_observation_ids = train_observation_ids + dev_observation_ids + test_state_ids\n",
    "\n",
    "hindi_pos_tagger = HMM(len(state_dict), len(observation_dict))\n",
    "hindi_pos_tagger.fit(full_state_ids, full_observation_ids)\n",
    "\n",
    "pickle.dump(hindi_pos_tagger, open('hindi_pos_tagger.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c4028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finally we will use the hindi_pos_tagger as a pre-processing step for our NER tagger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
